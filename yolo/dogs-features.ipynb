{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dogs feature data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dropout, Flatten, Dense, Conv2D, BatchNormalization, Activation, AveragePooling2D, concatenate, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.utils import Progbar, GeneratorEnqueuer\n",
    "from keras.applications import imagenet_utils\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_root = \"/Users/xuan/work/dataset/dogs/images\"\n",
    "annotations_root = \"/Users/xuan/work/dataset/dogs/annotations\"\n",
    "\n",
    "train_images_root = images_root + \"/train\"\n",
    "val_images_root = images_root + \"/val\"\n",
    "train_annotations_root = annotations_root + \"/train\"\n",
    "val_annotations_root = annotations_root + \"/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "def mkdirp(dir):\n",
    "  try:\n",
    "    os.mkdir(dir)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def load_base(model):\n",
    "  if model == \"vgg\" or model == \"vgg16\":\n",
    "      return applications.VGG16(weights='imagenet', include_top=False),  applications.vgg16.decode_predictions, applications.vgg16.preprocess_input\n",
    "  elif model == \"mobilenet\" or model == \"mn\":\n",
    "      return applications.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3)), applications.mobilenet.decode_predictions, applications.mobilenet.preprocess_input\n",
    "  elif model == \"resnet\" or model == \"resnet50\":\n",
    "      return applications.ResNet50(weights='imagenet', include_top=False), applications.resnet50.decode_predictions, applications.resnet50.preprocess_input\n",
    "  elif model == \"inceptionv3\" or model == \"inception\":\n",
    "      return applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)), applications.inception_v3.decode_predictions, applications.inception_v3.preprocess_input\n",
    "  else:\n",
    "      return None\n",
    "    \n",
    "def load_model(model):\n",
    "  if model == \"vgg\" or model == \"vgg16\":\n",
    "      return applications.VGG16(weights='imagenet'),  applications.vgg16.decode_predictions, applications.vgg16.preprocess_input\n",
    "  elif model == \"mobilenet\" or model == \"mn\":\n",
    "      return applications.MobileNet(weights='imagenet', input_shape=(224, 224, 3)), applications.mobilenet.decode_predictions, applications.mobilenet.preprocess_input\n",
    "  elif model == \"resnet\" or model == \"resnet50\":\n",
    "      return applications.ResNet50(weights='imagenet'), applications.resnet50.decode_predictions, applications.resnet50.preprocess_input\n",
    "  elif model == \"inceptionv3\" or model == \"inception\":\n",
    "      return applications.InceptionV3(weights='imagenet', input_shape=(224, 224, 3)), applications.inception_v3.decode_predictions, applications.inception_v3.preprocess_input\n",
    "  else:\n",
    "      return None\n",
    "\n",
    "def npy_file(basedir, prefix):\n",
    "  return \"{}/{}.npy\".format(basedir, prefix)\n",
    "\n",
    "def npy_file_x(basedir, prefix):\n",
    "  return npy_file(basedir, prefix + \".x\")\n",
    "\n",
    "def npy_file_y(basedir, prefix):\n",
    "  return npy_file(basedir, prefix + \".y\")\n",
    "\n",
    "\n",
    "def load_feature(dir, prefix):\n",
    "  feature_file = npy_file_x(dir, prefix)\n",
    "  label_file = npy_file_y(dir, prefix)\n",
    "  features = np.load(open(feature_file))\n",
    "  labels = np.load(open(label_file))\n",
    "  return features, labels\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "    filters = int(filters)\n",
    "    x = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=name + \"_conv\")(x)\n",
    "    x = BatchNormalization(scale=False, name=name + \"_bn\")(x)\n",
    "    x = Activation('relu', name=name)(x)\n",
    "    return x \n",
    "\n",
    "def incept(x, name, scale=1):\n",
    "    branch1x1 = conv2d_bn(x, 64 // scale, 1, 1, name = name + \"-1x1\")\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48 // scale , 1, 1, name = name + \"-5x5-1x1\")\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64 // scale, 5, 5, name = name + \"-5x5-5x5\")\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64 // scale, 1, 1, name = name + \"-3x3-1x1\")\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96 // scale, 3, 3, name = name + \"-3x3-3x3-1\")\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96 // scale, 3, 3, name = name + \"-3x3-3x3-2\")\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32 // scale, 1, 1, name = name + \"-pool\")\n",
    "    return concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        name= name + '-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo functions\n",
    "def anno_file_to_rect(anno_file):\n",
    "    tree = ET.parse(anno_file)\n",
    "    objs = tree.getroot().findall(\"object\")\n",
    "    boxes = [obj.find(\"bndbox\") for obj in objs]\n",
    "    return [(int(box.find(\"xmin\").text), \n",
    "              int(box.find(\"ymin\").text), \n",
    "              int(box.find(\"xmax\").text), \n",
    "              int(box.find(\"ymax\").text)) for box in boxes][0]\n",
    "\n",
    "def get_image_size(af):\n",
    "    tree = ET.parse(af)\n",
    "    objs = tree.getroot().findall(\"object\")\n",
    "    boxes = [obj.find(\"bndbox\") for obj in objs]\n",
    "    image_size_el = tree.getroot().find(\"size\")\n",
    "    return (float(image_size_el.find(\"width\").text), float(image_size_el.find(\"height\").text))\n",
    "\n",
    "## return 7 x 7 * 5(C, cx, cy, hx, hy)\n",
    "def anno_file_to_yolo_y(af, grid = (7 , 7)):\n",
    "    tree = ET.parse(af)\n",
    "    objs = tree.getroot().findall(\"object\")\n",
    "    boxes = [obj.find(\"bndbox\") for obj in objs]\n",
    "    image_size_el = tree.getroot().find(\"size\")\n",
    "    image_width = float(image_size_el.find(\"width\").text)\n",
    "    image_height = float(image_size_el.find(\"height\").text)\n",
    "    cell_width = image_width / grid[0]\n",
    "    cell_height = image_height / grid[1]\n",
    "    \n",
    "    rect = [(float(box.find(\"xmin\").text), \n",
    "          float(box.find(\"ymin\").text), \n",
    "          float(box.find(\"xmax\").text), \n",
    "          float(box.find(\"ymax\").text)) for box in boxes][0]\n",
    "\n",
    "    width = rect[2] - rect[0]\n",
    "    height = rect[3] - rect[1]\n",
    "    center_x = (rect[2] + rect[0]) / 2.0\n",
    "    center_y = (rect[3] + rect[1]) / 2.0\n",
    "    \n",
    "    # find which cell is the 1 one\n",
    "    cell_idx_x = int(center_x * grid[0] / image_width)\n",
    "    cell_idx_y = int(center_y * grid[1] / image_height)\n",
    "    \n",
    "    y1 = [1, \n",
    "          center_x / cell_width - cell_idx_x, \n",
    "          center_y / cell_height - cell_idx_y,\n",
    "          width / image_width,\n",
    "          height / image_height\n",
    "         ]\n",
    "    \n",
    "    y = np.zeros((5 * grid[0] * grid[1]), dtype=np.float16)\n",
    "    base = cell_idx_x  + cell_idx_y * grid[0]\n",
    "    y[base * 5: (base + 1) * 5] = y1\n",
    "    \n",
    "    return y\n",
    "  \n",
    "def img2data(image_file):\n",
    "  return np.array([img_to_array(load_img(image_file, target_size = (224, 224)))], dtype=np.float16)\n",
    "\n",
    "JPEG_EXT = \"JPEG\"\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class YoloDataGenerator(object):\n",
    "    'Generates image yolo from dataset'\n",
    "    def __init__(self, image_dir, annotation_dir, grid = (7,7), batch_size = 16, target_size = (224, 224)):\n",
    "        'Initialization'\n",
    "        self.image_dir = image_dir # image id list\n",
    "        self.annotation_dir = annotation_dir\n",
    "        ids = []\n",
    "        for clz in os.listdir(annotation_dir):\n",
    "            ids.append([clz + \"/\" + f for f in os.listdir(annotation_dir + \"/\" + clz)])\n",
    "        self.ids = flatten(ids)\n",
    "        self.steps = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.grid = grid\n",
    "\n",
    "    def generate(self):\n",
    "        while self.steps < len(self.ids) // self.batch_size:\n",
    "            ids = self.ids[self.steps * self.batch_size: (self.steps + 1) * self.batch_size]\n",
    "            image_files = [self.image_dir + \"/\" + id + \".\" + JPEG_EXT for id in ids]\n",
    "            anno_files = [self.annotation_dir + \"/\" + id for id in ids]\n",
    "            ys = [anno_file_to_yolo_y(af, self.grid) for af in anno_files]\n",
    "            xs = [image.img_to_array(image.load_img(image_file,target_size =  self.target_size)) for image_file in image_files]\n",
    "            self.steps += 1\n",
    "            yield np.array(xs, dtype=np.float16), np.array(ys, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the feature data\n",
    "round = .1\n",
    "nb_train_samples = NB_TRAIN_SAMPLES = int(16494 * round // 1) ## copy from split script\n",
    "nb_val_samples = NB_VAL_SAMPLES = int(4086 * round //1) ## copy from split script\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "n_classes = 120\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "base_model_name = \"inception\"\n",
    "GRIDS = {\"inception\": (5,5)}\n",
    "grid = GRIDS[base_model_name]\n",
    "\n",
    "mkdirp(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the feature map files if not yet generate before\n",
    "train_generator = YoloDataGenerator(train_images_root, train_annotations_root, grid = grid)\n",
    "val_generator = YoloDataGenerator(val_images_root, val_annotations_root, grid = grid)\n",
    "\n",
    "def generate_features(generator, processor, model, total_samples, output_dir, prefix, batch_size = 16):\n",
    "    steps = 1\n",
    "    enqueuer = GeneratorEnqueuer(generator)\n",
    "    feature_file = npy_file_x(output_dir, prefix)\n",
    "    label_file = npy_file_y(output_dir, prefix)\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    batch_nums = total_samples//batch_size\n",
    "    progbar = Progbar(target=batch_nums)\n",
    "    \n",
    "    enqueuer.start()\n",
    "    output_generator = enqueuer.get()\n",
    "    \n",
    "    for b in range(0, batch_nums):\n",
    "      batch_x, batch_labels = next(output_generator) ## 从原始数据中拉取一批数据\n",
    "      feature = model.predict(processor(batch_x), batch_size = batch_size) # 根据数据提取feature\n",
    "      all_labels.append(batch_labels)\n",
    "      all_features.append(feature)\n",
    "      steps += 1\n",
    "      progbar.update(steps)\n",
    "\n",
    "    all_features = np.concatenate(all_features)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    print \"Round {} finished, saved to features {} (shapes {}) labels {} (shape {})\".format(round, feature_file, all_features.shape, label_file, all_labels.shape)\n",
    "    \n",
    "    np.save(open(feature_file, 'w'), all_features)\n",
    "    np.save(open(label_file, 'w'), all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the base model done inception\n",
      "25/25 [==============================] - 64s 3s/step\n",
      "26/25 [===============================] - 66s 3s/step\n",
      "Round 0.1 finished, saved to features inception/val-dogs-120.x.npy (shapes (400, 5, 5, 2048)) labels inception/val-dogs-120.y.npy (shape (400, 125))\n",
      "103/103 [==============================] - 265s 3s/step\n",
      "104/103 [==============================] - 268s 3s/step\n",
      "Round 0.1 finished, saved to features inception/train-dogs-120.x.npy (shapes (1648, 5, 5, 2048)) labels inception/train-dogs-120.y.npy (shape (1648, 125))\n"
     ]
    }
   ],
   "source": [
    "# generate the features\n",
    "base_model, base_decoder, processor = load_base(base_model_name)\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "print \"load the base model done\", base_model_name\n",
    "generate_features(val_generator.generate(), processor, base_model, nb_val_samples, base_model_name, \"val-dogs-120\")\n",
    "generate_features(train_generator.generate(), processor, base_model, nb_train_samples, base_model_name, \"train-dogs-120\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
