{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object: 验证lambda layer和loss的等价性\n",
    "## 构造一个简单的数据集，然后使用lambda和loss分别fit，看weight是否等价\n",
    "\n",
    "### KR1: 把loss作为layer还是loss function看不出差异点\n",
    "### KR2: 经常train坏掉，为什么？overfit?\n",
    "### KR3: sigmoid和tanh为什么不工作？和relu的差异很大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KR2 问题：  ~~注意到一些layer没有被initialized到，所以导致无法被train到，所以导致了~~ 不对，一些dense的bias确实是init成0的。和overfit没关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KR1: loss作为layer和loss fn结果非常类似, 在activation=tanh的情况下表现明显，tanh和relu不同的是tanh很稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KR3: tanh & sigmoid的activation，被缩在一起，基本上是一个直线，underfit很严重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential, Input\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Model, Sequential, Input\n",
    "from keras.layers import Dense, Lambda, Dropout\n",
    "from keras import losses\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate the X\n",
    "def sample(size):\n",
    "    X = np.random.random(size) * 2.0 - 1.0\n",
    "    noise = np.random.random(size) / 10.0\n",
    "    Y = X * X + 1.0 + noise\n",
    "    return X.reshape(size, 1),Y.reshape(size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape (128, 1)\n",
      "train y shape (128, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKbiLiJSQgruISAn9f239zK6jdKSOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1163df0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m=64 * 2\n",
    "train_X, train_y = sample(m)\n",
    "val_X, val_y = sample(m/3)\n",
    "layers = [2]\n",
    "activation = \"relu\"\n",
    "\n",
    "# plot the sample and val data\n",
    "plt.plot(train_X, train_y, \"bo\")\n",
    "plt.plot(val_X, val_y, \"ro\")\n",
    "print \"train X shape\", train_X.shape\n",
    "print \"train y shape\", train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fit by loss\n",
    "x_input = Input(shape=(1,))\n",
    "x = x_input\n",
    "for num in layers:\n",
    "    x = Dense(num, activation=activation)(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "model = Model(x_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 42 samples\n",
      "Epoch 1/1000\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 2/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 3/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 4/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 5/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 6/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 7/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 8/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 9/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 10/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 11/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 12/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 13/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 14/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 15/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 16/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 17/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 18/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 19/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 20/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 21/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 22/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 23/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 24/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 25/1000\n",
      "128/128 [==============================] - 0s 76us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 26/1000\n",
      "128/128 [==============================] - 0s 83us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 27/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 28/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 29/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 30/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 31/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 32/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 33/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 34/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 35/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 36/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 37/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 38/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 39/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 40/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 41/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 42/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 43/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 44/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 45/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 46/1000\n",
      "128/128 [==============================] - 0s 73us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 47/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 48/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 49/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 50/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 51/1000\n",
      "128/128 [==============================] - 0s 71us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 52/1000\n",
      "128/128 [==============================] - 0s 44us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 53/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 54/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 55/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 56/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 57/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 58/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 59/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 60/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 61/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 62/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 63/1000\n",
      "128/128 [==============================] - 0s 77us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 64/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 65/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 66/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.071 - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 67/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 68/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 69/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 70/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 71/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 72/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 73/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 74/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 75/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 76/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 77/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 78/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 80/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 81/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 82/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 83/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 84/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 85/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 86/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 87/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 88/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 89/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 90/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 91/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 92/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 93/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 94/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 95/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 96/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 97/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 98/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 99/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 100/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 101/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 102/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 103/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 104/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 105/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 106/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 107/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 108/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 109/1000\n",
      "128/128 [==============================] - 0s 76us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 110/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 111/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 112/1000\n",
      "128/128 [==============================] - 0s 75us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 113/1000\n",
      "128/128 [==============================] - 0s 73us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 114/1000\n",
      "128/128 [==============================] - 0s 72us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 115/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 116/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 117/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 118/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 119/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 120/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 121/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 122/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 123/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 124/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 125/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 126/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 127/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 128/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 129/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 130/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 131/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 132/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 133/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 134/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 135/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 136/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 137/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 138/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 139/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 140/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 141/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 142/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 143/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 144/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.724 - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 145/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 146/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 147/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 148/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 149/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 150/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 151/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 152/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 153/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 154/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 155/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 156/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 158/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 159/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 160/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 161/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 162/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 163/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 164/1000\n",
      "128/128 [==============================] - 0s 77us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 165/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 166/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 167/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 168/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 169/1000\n",
      "128/128 [==============================] - 0s 76us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 170/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 171/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 172/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 173/1000\n",
      "128/128 [==============================] - 0s 75us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 174/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 175/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 176/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 177/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 178/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 179/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 180/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 181/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 182/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 183/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 184/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 185/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 186/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 187/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 188/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 189/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 190/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 191/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 192/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 193/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 194/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 195/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 196/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 197/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 198/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 199/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 200/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 201/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 202/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 203/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 204/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 205/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 206/1000\n",
      "128/128 [==============================] - 0s 85us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 207/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 208/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 209/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 210/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 211/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 212/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 213/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 214/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 215/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 216/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 217/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 218/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 219/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 220/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 221/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 222/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 223/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 224/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 225/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 226/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 227/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 228/1000\n",
      "128/128 [==============================] - 0s 73us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 229/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 230/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 231/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 232/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 233/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 234/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 236/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 237/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 238/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 239/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 240/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 241/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 242/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 243/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 244/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 245/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 246/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 247/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 248/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 249/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 250/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 251/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 252/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 253/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 254/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 255/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 256/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 257/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 258/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 259/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 260/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 261/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 262/1000\n",
      "128/128 [==============================] - 0s 72us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 263/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 264/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 265/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 266/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 267/1000\n",
      "128/128 [==============================] - 0s 73us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 268/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 269/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 270/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 271/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 272/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 273/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 274/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 275/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 276/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 277/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 278/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 279/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 280/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 281/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 282/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 283/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 284/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 285/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 286/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 287/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 288/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 289/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 290/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 291/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 292/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 293/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 294/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 295/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 296/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 297/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 298/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 299/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 300/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 301/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 302/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 303/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 304/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 305/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 306/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 307/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 308/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 309/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 310/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 311/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 312/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 313/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 314/1000\n",
      "128/128 [==============================] - 0s 73us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 315/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 316/1000\n",
      "128/128 [==============================] - 0s 75us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 317/1000\n",
      "128/128 [==============================] - 0s 75us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 318/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 319/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 320/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 321/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 322/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 323/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 324/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 325/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 326/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 327/1000\n",
      "128/128 [==============================] - 0s 74us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 328/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 329/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 330/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 331/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 332/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 333/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 334/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 335/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 336/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 337/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 338/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 339/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 340/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 341/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 342/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 343/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 344/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 345/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 346/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 347/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 348/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 349/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 350/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 351/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 352/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 353/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 354/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 355/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.154 - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 356/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.036 - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 357/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 358/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 359/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 360/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 361/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 362/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 363/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 364/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 365/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 366/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 367/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 368/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 369/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 370/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 371/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 372/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 373/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 374/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 375/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.865 - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 376/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.926 - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 377/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 378/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.994 - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 379/1000\n",
      "128/128 [==============================] - 0s 77us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 380/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 381/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 382/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 383/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 384/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 385/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 386/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 387/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 388/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 389/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 390/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - ETA: 0s - loss: 2.202 - 0s 74us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 391/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 392/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 393/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 394/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 395/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 396/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 397/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 398/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 399/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 400/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 401/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 402/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 403/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 404/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 405/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 406/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 407/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 408/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 409/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 410/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.005 - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 411/1000\n",
      "128/128 [==============================] - 0s 80us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 412/1000\n",
      "128/128 [==============================] - 0s 76us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 413/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 414/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 415/1000\n",
      "128/128 [==============================] - 0s 76us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 416/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.949 - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 417/1000\n",
      "128/128 [==============================] - 0s 72us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 418/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 419/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 420/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 421/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 422/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 423/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 424/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 425/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 426/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 427/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 428/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 429/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 430/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 431/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 432/1000\n",
      "128/128 [==============================] - 0s 72us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 433/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 434/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 435/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 436/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 437/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 438/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 439/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 440/1000\n",
      "128/128 [==============================] - 0s 71us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 441/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 442/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 443/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 444/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 445/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 446/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 447/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 448/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 449/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 450/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 451/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 452/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 453/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 454/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 455/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 456/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 457/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 458/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 459/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 460/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 461/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 462/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 463/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 464/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 465/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 466/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 467/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 468/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 469/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 470/1000\n",
      "128/128 [==============================] - 0s 43us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 471/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 472/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 473/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 474/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 475/1000\n",
      "128/128 [==============================] - 0s 77us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 476/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 477/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 478/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 479/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.801 - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 480/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 481/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 482/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 483/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 484/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 485/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 486/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 487/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 488/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 489/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 490/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 491/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 492/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 493/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 494/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 495/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 496/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 497/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 498/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 499/1000\n",
      "128/128 [==============================] - 0s 44us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 500/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 501/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 502/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 503/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 504/1000\n",
      "128/128 [==============================] - 0s 109us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 505/1000\n",
      "128/128 [==============================] - 0s 79us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 506/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 507/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 508/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 509/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 510/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 511/1000\n",
      "128/128 [==============================] - 0s 68us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 512/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 513/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 514/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 515/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 516/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 517/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 518/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.019 - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 519/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 520/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 521/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 522/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 523/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 524/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 525/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 526/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 527/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 528/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 529/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 530/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 531/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 532/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 533/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 534/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 535/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 536/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 537/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 538/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 539/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 540/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 541/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 542/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 543/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 544/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 545/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 546/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 547/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 548/1000\n",
      "128/128 [==============================] - 0s 44us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 549/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 550/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 551/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 552/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 553/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 554/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 555/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 556/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 557/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 558/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 559/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 560/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 561/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 562/1000\n",
      "128/128 [==============================] - 0s 83us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 563/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 564/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 565/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 566/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 567/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 568/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 569/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 570/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 571/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 572/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 573/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 574/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 575/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 576/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 577/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 578/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 579/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 580/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 581/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 582/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 583/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 584/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 585/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 586/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 587/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 588/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 589/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 590/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 591/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 592/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 593/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 594/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 595/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 596/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 597/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 598/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 599/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 600/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 601/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 602/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 603/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 604/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 605/1000\n",
      "128/128 [==============================] - 0s 45us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 606/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 607/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 608/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.867 - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 609/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 610/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 611/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 612/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 613/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 614/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 615/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 616/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 617/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 618/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 619/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 620/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 621/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 622/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 623/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 624/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 625/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 626/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 627/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 628/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 629/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 630/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 631/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 632/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 633/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 634/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 635/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 636/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 637/1000\n",
      "128/128 [==============================] - 0s 45us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 638/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 639/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 640/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 641/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 642/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 643/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 644/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 645/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 646/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 647/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 648/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 649/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 650/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 651/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 652/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 653/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 654/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 655/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 656/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 657/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 658/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 659/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 660/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 661/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 662/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 663/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 664/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 665/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 666/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 667/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 668/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 669/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 670/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 671/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 672/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 673/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 674/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 675/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 676/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 677/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 678/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 679/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 680/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 681/1000\n",
      "128/128 [==============================] - 0s 69us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 682/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 683/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 684/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 685/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 686/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 687/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 688/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 689/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 690/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 691/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 692/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 693/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 694/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 695/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 696/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 697/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 698/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 699/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 700/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 701/1000\n",
      "128/128 [==============================] - 0s 71us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 702/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 703/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 704/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 705/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 706/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 707/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 708/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 709/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 710/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 711/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 712/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 713/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 714/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 715/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 716/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 717/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 718/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 719/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 720/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 721/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 722/1000\n",
      "128/128 [==============================] - 0s 71us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 723/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 724/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 725/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 726/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 727/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.922 - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 728/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 729/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 730/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 731/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 732/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 733/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 734/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 735/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 736/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 737/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 738/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 739/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 740/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 741/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 742/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 743/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 744/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 745/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 746/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.873 - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 747/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 748/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 749/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 750/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 751/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 752/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.857 - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 753/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 754/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 755/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 756/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 757/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 758/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 759/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 760/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 761/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 762/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 763/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 764/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 765/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 766/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 767/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 768/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 769/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 770/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 771/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 772/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 773/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 774/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 775/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 776/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 777/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 778/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 779/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 780/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 781/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 782/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 783/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 784/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 785/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 786/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 787/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 788/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 789/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 790/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 791/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 792/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 793/1000\n",
      "128/128 [==============================] - 0s 65us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 794/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 795/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 796/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 797/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 798/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 799/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 800/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 801/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 802/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 803/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 804/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 805/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 806/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 807/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 808/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 809/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 810/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 811/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 812/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 813/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 814/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 815/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 816/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 817/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 818/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 819/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 820/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 821/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 822/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 823/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 824/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 825/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 826/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 827/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 828/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 829/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 830/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 2.339 - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 831/1000\n",
      "128/128 [==============================] - 0s 72us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 832/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 833/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 834/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 835/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 836/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 837/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 838/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 839/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 840/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 841/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 842/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 843/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 844/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 845/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 846/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 847/1000\n",
      "128/128 [==============================] - 0s 70us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 848/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 849/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 850/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 851/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 852/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 853/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 854/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 855/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 856/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 857/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 858/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 859/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 860/1000\n",
      "128/128 [==============================] - 0s 67us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 861/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 862/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 863/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 864/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 865/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 866/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 867/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 868/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 869/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 870/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 871/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 872/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 873/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 874/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 875/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 876/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 877/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 878/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 879/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 880/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 881/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 882/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 883/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 884/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 885/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 886/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 887/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 888/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 889/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 890/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 891/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 892/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 893/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 894/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 895/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 896/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 897/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 898/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 899/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 900/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 901/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 902/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 903/1000\n",
      "128/128 [==============================] - 0s 66us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 904/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 905/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 906/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 907/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 908/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 909/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 910/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 911/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 912/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 913/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 914/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 915/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 916/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 917/1000\n",
      "128/128 [==============================] - 0s 63us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 918/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 919/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 920/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 921/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 922/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 923/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 924/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 925/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 926/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 927/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 928/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 929/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 930/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 931/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 932/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 933/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 934/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 935/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 936/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 937/1000\n",
      "128/128 [==============================] - 0s 61us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 938/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 939/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 940/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 941/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 942/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 943/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 944/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 945/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 946/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 947/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 948/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 949/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 950/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 951/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 952/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 953/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 954/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 955/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 956/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 957/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 958/1000\n",
      "128/128 [==============================] - 0s 55us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 959/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 960/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 961/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 962/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 963/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.579 - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 964/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 965/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 966/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 967/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 968/1000\n",
      "128/128 [==============================] - 0s 60us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 969/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 970/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 971/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.967 - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 972/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 973/1000\n",
      "128/128 [==============================] - 0s 46us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 974/1000\n",
      "128/128 [==============================] - 0s 57us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 975/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 976/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 977/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 978/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 979/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 980/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 981/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 982/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 983/1000\n",
      "128/128 [==============================] - 0s 53us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 984/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 985/1000\n",
      "128/128 [==============================] - 0s 47us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 986/1000\n",
      "128/128 [==============================] - 0s 58us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 987/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 988/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 989/1000\n",
      "128/128 [==============================] - 0s 56us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 990/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 991/1000\n",
      "128/128 [==============================] - 0s 51us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 992/1000\n",
      "128/128 [==============================] - 0s 64us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 993/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 994/1000\n",
      "128/128 [==============================] - 0s 62us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 995/1000\n",
      "128/128 [==============================] - 0s 52us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 996/1000\n",
      "128/128 [==============================] - 0s 48us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 997/1000\n",
      "128/128 [==============================] - 0s 59us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 998/1000\n",
      "128/128 [==============================] - 0s 50us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 999/1000\n",
      "128/128 [==============================] - 0s 54us/step - loss: 2.0059 - val_loss: 1.8300\n",
      "Epoch 1000/1000\n",
      "128/128 [==============================] - 0s 49us/step - loss: 2.0059 - val_loss: 1.8300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1163ed290>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mse')\n",
    "model.fit(train_X,\n",
    "          train_y,\n",
    "          epochs=1000, \n",
    "          batch_size=32,\n",
    "          shuffle=True,\n",
    "          validation_data = (val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3ImIUuj/A2eO7jZjcEEVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1168e0c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_ = model.predict(train_X)\n",
    "\n",
    "plt.plot(train_X, train_y, \"go\")\n",
    "plt.plot(train_X, y_, \"rx\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            4           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            3           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_loss (Lambda)              (None, 1)            0           dense_4[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7\n",
      "Trainable params: 7\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  del sys.path[0]\n",
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"yo..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# fit by a lambda layer\n",
    "x_input = Input(shape=(1,))\n",
    "x = x_input\n",
    "for num in layers:\n",
    "    x = Dense(num, activation=activation)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "\n",
    "def loss_fn(args):\n",
    "    (y_,y) = args\n",
    "    return losses.mean_squared_error(y, y_)\n",
    "\n",
    "loss_model_predictor = Model(input = x_input, output = x)\n",
    "\n",
    "y_input = Input(shape=(1,))\n",
    "\n",
    "loss_layer = Lambda(loss_fn, \n",
    "            output_shape=(1, ),\n",
    "            name='yolo_loss',\n",
    "            arguments={})([x, y_input])\n",
    "\n",
    "loss_model = Model(input = ([x_input, y_input]), outputs = loss_layer)\n",
    "\n",
    "loss_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 42 samples\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 3ms/step - loss: 1.3978 - val_loss: 1.1829\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 69us/step - loss: 1.4216 - val_loss: 1.1737\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 59us/step - loss: 1.4351 - val_loss: 1.1649\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 1.4528 - val_loss: 1.1563\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 1.4441 - val_loss: 1.1470\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.4120 - val_loss: 1.1382\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 1.2742 - val_loss: 1.1289\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 79us/step - loss: 1.3428 - val_loss: 1.1201\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 1.3420 - val_loss: 1.1114\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 1.4245 - val_loss: 1.1032\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 1.3521 - val_loss: 1.0947\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 1.4204 - val_loss: 1.0865\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 1.3882 - val_loss: 1.0783\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 1.3011 - val_loss: 1.0701\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 1.3560 - val_loss: 1.0620\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 1.2800 - val_loss: 1.0539\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 1.3374 - val_loss: 1.0460\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 0s 66us/step - loss: 1.2413 - val_loss: 1.0381\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 0s 53us/step - loss: 1.3309 - val_loss: 1.0304\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.1674 - val_loss: 1.0226\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.2110 - val_loss: 1.0152\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 0s 76us/step - loss: 1.3070 - val_loss: 1.0077\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.896 - 0s 58us/step - loss: 1.2452 - val_loss: 1.0002\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.1902 - val_loss: 0.9927\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.2567 - val_loss: 0.9849\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 0s 59us/step - loss: 1.2960 - val_loss: 0.9773\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 1.2392 - val_loss: 0.9700\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 1.1164 - val_loss: 0.9629\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 1.3072 - val_loss: 0.9555\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 1.2853 - val_loss: 0.9482\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 1.2361 - val_loss: 0.9412\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 0s 73us/step - loss: 1.3018 - val_loss: 0.9342\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 1.3229 - val_loss: 0.9272\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.2011 - val_loss: 0.9207\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 0s 79us/step - loss: 1.2005 - val_loss: 0.9143\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 0s 59us/step - loss: 1.1634 - val_loss: 0.9080\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.1629 - val_loss: 0.9016\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 0s 71us/step - loss: 1.2192 - val_loss: 0.8953\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 1.1238 - val_loss: 0.8892\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 0s 95us/step - loss: 1.2280 - val_loss: 0.8830\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.2237 - val_loss: 0.8770\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 0s 80us/step - loss: 1.1524 - val_loss: 0.8713\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 1.0646 - val_loss: 0.8657\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.2070 - val_loss: 0.8597\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 1.1294 - val_loss: 0.8540\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 1.0692 - val_loss: 0.8487\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 1.1527 - val_loss: 0.8431\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.0904 - val_loss: 0.8380\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 0s 59us/step - loss: 1.0987 - val_loss: 0.8326\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 1.1592 - val_loss: 0.8272\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 1.1823 - val_loss: 0.8217\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 0s 69us/step - loss: 1.1256 - val_loss: 0.8164\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.0267 - val_loss: 0.8115\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 1.1124 - val_loss: 0.8066\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 1.0489 - val_loss: 0.8020\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.0907 - val_loss: 0.7972\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 1.0844 - val_loss: 0.7923\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 1.0752 - val_loss: 0.7876\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 1.1069 - val_loss: 0.7827\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 0s 56us/step - loss: 1.0715 - val_loss: 0.7781\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.0036 - val_loss: 0.7740\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.0520 - val_loss: 0.7697\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 1.1144 - val_loss: 0.7649\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 0s 57us/step - loss: 1.0040 - val_loss: 0.7608\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 0s 59us/step - loss: 1.0453 - val_loss: 0.7566\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 0s 72us/step - loss: 1.1591 - val_loss: 0.7519\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.9986 - val_loss: 0.7477\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 1.0506 - val_loss: 0.7435\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 0s 94us/step - loss: 1.0430 - val_loss: 0.7394\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 1.0487 - val_loss: 0.7354\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 0s 66us/step - loss: 1.0518 - val_loss: 0.7312\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 1.0140 - val_loss: 0.7273\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 1.0040 - val_loss: 0.7234\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 0.9715 - val_loss: 0.7196\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 0s 79us/step - loss: 0.9667 - val_loss: 0.7159\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.9761 - val_loss: 0.7123\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.9265 - val_loss: 0.7090\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 0s 65us/step - loss: 0.8977 - val_loss: 0.7056\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 1.1569 - val_loss: 0.7013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "128/128 [==============================] - 0s 67us/step - loss: 0.9576 - val_loss: 0.6977\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.0295 - val_loss: 0.6939\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 0s 83us/step - loss: 0.9933 - val_loss: 0.6905\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 0s 69us/step - loss: 0.9813 - val_loss: 0.6871\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 0.9538 - val_loss: 0.6839\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.731 - 0s 72us/step - loss: 0.9455 - val_loss: 0.6805\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 0s 68us/step - loss: 0.9744 - val_loss: 0.6771\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 0s 61us/step - loss: 1.0165 - val_loss: 0.6736\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.9301 - val_loss: 0.6704\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 1.0080 - val_loss: 0.6671\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 0s 72us/step - loss: 0.9603 - val_loss: 0.6638\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 0.9503 - val_loss: 0.6605\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.9064 - val_loss: 0.6573\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.9920 - val_loss: 0.6539\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 0s 62us/step - loss: 0.9232 - val_loss: 0.6509\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 0s 64us/step - loss: 0.9919 - val_loss: 0.6475\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 0s 60us/step - loss: 0.9312 - val_loss: 0.6445\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.8192 - val_loss: 0.6419\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 0s 63us/step - loss: 0.9418 - val_loss: 0.6387\n",
      "Epoch 99/100\n",
      "128/128 [==============================] - 0s 70us/step - loss: 0.9147 - val_loss: 0.6355\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 0s 58us/step - loss: 0.8728 - val_loss: 0.6326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114f3af90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the loss layer model\n",
    "loss_model.compile(optimizer='adadelta',\n",
    "              loss=lambda y, y_: y_)\n",
    "loss_model.fit([train_X, train_y], np.zeros(len(train_y)),\n",
    "               epochs=100, \n",
    "               batch_size=32,\n",
    "               shuffle=True, \n",
    "               validation_data = ([val_X, val_y], np.zeros(len(val_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "[array([[-0.00965631, -0.56386346]], dtype=float32), array([ 0.,  0.], dtype=float32)]\n",
      "\n",
      "[array([[-0.83686173],\n",
      "       [-0.47128063]], dtype=float32), array([ 0.], dtype=float32)]\n",
      "\n",
      "debug the loss model:\n",
      "[]\n",
      "\n",
      "[array([[ 0.62272662,  1.29169071]], dtype=float32), array([-0.07313211,  0.11720413], dtype=float32)]\n",
      "\n",
      "[]\n",
      "\n",
      "[array([[ 0.01041435],\n",
      "       [ 1.08175492]], dtype=float32), array([ 0.25574657], dtype=float32)]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print l.get_weights()\n",
    "    print \"\"\n",
    "\n",
    "print \"debug the loss model:\"\n",
    "for l in loss_model.layers:\n",
    "    print l.get_weights()\n",
    "    print \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "print model.predict(np.array([-1., -.5, 0., 0.5, 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 weights [[-0.00965631 -0.56386346]] [ 0.  0.]\n",
      "layer 2 weights [[-0.83686173]\n",
      " [-0.47128063]] [ 0.]\n",
      "\n",
      "after first layer [[ 0.00965631  0.56386346]\n",
      " [ 0.00482816  0.28193173]\n",
      " [ 0.          0.        ]\n",
      " [-0.00482816 -0.28193173]\n",
      " [-0.00965631 -0.56386346]]\n",
      "after relu layer [[ 0.00965631  0.56386346]\n",
      " [ 0.00482816  0.28193173]\n",
      " [ 0.          0.        ]\n",
      " [-0.         -0.        ]\n",
      " [-0.         -0.        ]]\n",
      "\n",
      "after second layer [[-0.27381892]\n",
      " [-0.13690946]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "after relu layer [[-0.]\n",
      " [-0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "l1, b1 = model.layers[1].get_weights()\n",
    "l2, b2 = model.layers[2].get_weights()\n",
    "print \"layer 1 weights\", l1, b1\n",
    "print \"layer 2 weights\", l2, b2\n",
    "print \"\" \n",
    "\n",
    "x = np.array([[-1.], [-.5], [0.], [0.5], [1.]])\n",
    "x_ = np.matmul(x, l1) + b1\n",
    "print \"after first layer\", x_\n",
    "\n",
    "x_ = x_ * (x_ > 0)\n",
    "print \"after relu layer\", x_\n",
    "\n",
    "print \"\"\n",
    "x_ = np.matmul(x_, l2) + b2\n",
    "print \"after second layer\", x_\n",
    "x_ = x_ * (x_ > 0)\n",
    "print \"after relu layer\", x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate the models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1153ccfd0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAOVCAYAAABtTm0hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114d91dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model by plot them\n",
    "print \"evaluate the models\"\n",
    "pred_train_y = model.predict(train_X)\n",
    "_, axs = plt.subplots(3, 2, figsize = (16, 16))\n",
    "ax = axs[0][0]\n",
    "ax.set_title(\"common model - train\")\n",
    "ax.plot(train_X, train_y, \"ro\")\n",
    "ax.plot(train_X, pred_train_y, \"bx\")\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title(\"common model - val\")\n",
    "pred_val_y = model.predict(val_X)\n",
    "ax.plot(val_X, val_y, \"ro\")\n",
    "ax.plot(val_X, pred_val_y, \"bx\")\n",
    "\n",
    "pred_train_y = loss_model_predictor.predict(train_X)\n",
    "ax = axs[1][0]\n",
    "ax.set_title(\"loss model - train\")\n",
    "ax.plot(train_X, train_y, \"ro\")\n",
    "ax.plot(train_X, pred_train_y, \"bx\")\n",
    "\n",
    "ax = axs[1][1]\n",
    "pred_val_y = loss_model_predictor.predict(val_X)\n",
    "ax.set_title(\"loss model - val\")\n",
    "ax.plot(val_X, val_y, \"ro\")\n",
    "ax.plot(val_X, pred_val_y, \"bx\")\n",
    "\n",
    "def random_predictor():\n",
    "    x_input = Input(shape=(1,))\n",
    "    x = x_input\n",
    "    for num in layers:\n",
    "        x = Dense(num, activation='tanh')(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    predictor = Model(x_input, x)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "rand_model_predictor = random_predictor()\n",
    "\n",
    "pred_train_y = rand_model_predictor.predict(train_X)\n",
    "ax = axs[2][0]\n",
    "ax.set_title(\"random model - train\")\n",
    "ax.plot(train_X, train_y, \"ro\")\n",
    "ax.plot(train_X, pred_train_y, \"bx\")\n",
    "\n",
    "ax = axs[2][1]\n",
    "pred_val_y = rand_model_predictor.predict(val_X)\n",
    "ax.set_title(\"random model - val\")\n",
    "ax.plot(val_X, val_y, \"ro\")\n",
    "ax.plot(val_X, pred_val_y, \"bx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
